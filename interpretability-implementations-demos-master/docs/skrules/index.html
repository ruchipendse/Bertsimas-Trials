<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
<meta name="generator" content="pdoc 0.7.2" />
<title>imodels.skrules API documentation</title>
<meta name="description" content="[skope-rules](https://github.com/scikit-learn-contrib/skope-rules) (based on [this implementation](https://github.com/scikit-learn-contrib/skope-rules))" />
<link href='https://cdnjs.cloudflare.com/ajax/libs/normalize/8.0.0/normalize.min.css' rel='stylesheet'>
<link href='https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/8.0.0/sanitize.min.css' rel='stylesheet'>
<link href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/github.min.css" rel="stylesheet">
<style>.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:30px;overflow:hidden}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:105%}a{color:#058;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#e82}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900}pre code{background:#f8f8f8;font-size:.8em;line-height:1.4em}code{background:#f2f2f1;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{background:#f8f8f8;border:0;border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0;padding:1ex}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{font-weight:bold}#index h4 + ul{margin-bottom:.6em}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-weight:bold;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}.admonition{padding:.1em .5em;margin-bottom:1em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.item .name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1.5em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>imodels.skrules</code></h1>
</header>
<section id="section-intro">
<p><a href="https://github.com/scikit-learn-contrib/skope-rules">skope-rules</a> (based on <a href="https://github.com/scikit-learn-contrib/skope-rules">this implementation</a>)</p>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">&#39;&#39;&#39;[skope-rules](https://github.com/scikit-learn-contrib/skope-rules) (based on [this implementation](https://github.com/scikit-learn-contrib/skope-rules))
&#39;&#39;&#39;

from .skope_rules import SkopeRules
from .rule import Rule, replace_feature_name

__all__ = [&#39;SkopeRules&#39;, &#39;Rule&#39;]</code></pre>
</details>
</section>
<section>
<h2 class="section-title" id="header-submodules">Sub-modules</h2>
<dl>
<dt><code class="name"><a title="imodels.skrules.datasets" href="datasets/index.html">imodels.skrules.datasets</a></code></dt>
<dd>
<section class="desc"></section>
</dd>
<dt><code class="name"><a title="imodels.skrules.rule" href="rule.html">imodels.skrules.rule</a></code></dt>
<dd>
<section class="desc"></section>
</dd>
<dt><code class="name"><a title="imodels.skrules.skope_rules" href="skope_rules.html">imodels.skrules.skope_rules</a></code></dt>
<dd>
<section class="desc"></section>
</dd>
<dt><code class="name"><a title="imodels.skrules.tests" href="tests/index.html">imodels.skrules.tests</a></code></dt>
<dd>
<section class="desc"></section>
</dd>
</dl>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-classes">Classes</h2>
<dl>
<dt id="imodels.skrules.Rule"><code class="flex name class">
<span>class <span class="ident">Rule</span></span>
<span>(</span><span>rule, args=None)</span>
</code></dt>
<dd>
<section class="desc"><p>An object modelling a logical rule and add factorization methods.
It is used to simplify rules and deduplicate them.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><a title="imodels.skrules.rule" href="rule.html"><code>imodels.skrules.rule</code></a></strong> :&ensp;<code>str</code></dt>
<dd>The logical rule that is interpretable by a pandas query.</dd>
<dt><strong><code>args</code></strong> :&ensp;<code>object</code>, optional</dt>
<dd>Arguments associated to the rule, it is not used for factorization
but it takes part of the output when the rule is converted to an array.</dd>
</dl></section>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class Rule:
    &#34;&#34;&#34; An object modelling a logical rule and add factorization methods.
    It is used to simplify rules and deduplicate them.

    Parameters
    ----------

    rule : str
        The logical rule that is interpretable by a pandas query.

    args : object, optional
        Arguments associated to the rule, it is not used for factorization
        but it takes part of the output when the rule is converted to an array.
    &#34;&#34;&#34;

    def __init__(self, rule, args=None):
        self.rule = rule
        self.args = args
        self.terms = [t.split(&#39; &#39;) for t in self.rule.split(&#39; and &#39;)]
        self.agg_dict = {}
        self.factorize()
        self.rule = str(self)

    def __eq__(self, other):
        return self.agg_dict == other.agg_dict

    def __hash__(self):
        # FIXME : Easier method ?
        return hash(tuple(sorted(((i, j) for i, j in self.agg_dict.items()))))

    def factorize(self):
        for feature, symbol, value in self.terms:
            if (feature, symbol) not in self.agg_dict:
                if symbol != &#39;==&#39;:
                    self.agg_dict[(feature, symbol)] = str(float(value))
                else:
                    self.agg_dict[(feature, symbol)] = value
            else:
                if symbol[0] == &#39;&lt;&#39;:
                    self.agg_dict[(feature, symbol)] = str(min(
                                float(self.agg_dict[(feature, symbol)]),
                                float(value)))
                elif symbol[0] == &#39;&gt;&#39;:
                    self.agg_dict[(feature, symbol)] = str(max(
                                float(self.agg_dict[(feature, symbol)]),
                                float(value)))
                else:  # Handle the c0 == c0 case
                    self.agg_dict[(feature, symbol)] = value

    def __iter__(self):
        yield str(self)
        yield self.args

    def __repr__(self):
        return &#39; and &#39;.join([&#39; &#39;.join(
                [feature, symbol, str(self.agg_dict[(feature, symbol)])])
                for feature, symbol in sorted(self.agg_dict.keys())
                ])</code></pre>
</details>
<h3>Methods</h3>
<dl>
<dt id="imodels.skrules.Rule.factorize"><code class="name flex">
<span>def <span class="ident">factorize</span></span>(<span>self)</span>
</code></dt>
<dd>
<section class="desc"></section>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def factorize(self):
    for feature, symbol, value in self.terms:
        if (feature, symbol) not in self.agg_dict:
            if symbol != &#39;==&#39;:
                self.agg_dict[(feature, symbol)] = str(float(value))
            else:
                self.agg_dict[(feature, symbol)] = value
        else:
            if symbol[0] == &#39;&lt;&#39;:
                self.agg_dict[(feature, symbol)] = str(min(
                            float(self.agg_dict[(feature, symbol)]),
                            float(value)))
            elif symbol[0] == &#39;&gt;&#39;:
                self.agg_dict[(feature, symbol)] = str(max(
                            float(self.agg_dict[(feature, symbol)]),
                            float(value)))
            else:  # Handle the c0 == c0 case
                self.agg_dict[(feature, symbol)] = value</code></pre>
</details>
</dd>
</dl>
</dd>
<dt id="imodels.skrules.SkopeRules"><code class="flex name class">
<span>class <span class="ident">SkopeRules</span></span>
<span>(</span><span>feature_names=None, precision_min=0.5, recall_min=0.01, n_estimators=10, max_samples=0.8, max_samples_features=1.0, bootstrap=False, bootstrap_features=False, max_depth=3, max_depth_duplication=None, max_features=1.0, min_samples_split=2, n_jobs=1, random_state=None, verbose=0)</span>
</code></dt>
<dd>
<section class="desc"><p>An easy-interpretable classifier optimizing simple logical rules.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>feature_names</code></strong> :&ensp;<code>list</code> of <code>str</code>, optional</dt>
<dd>The names of each feature to be used for returning rules in string
format.</dd>
<dt><strong><code>precision_min</code></strong> :&ensp;<code>float</code>, optional (default=<code>0.5</code>)</dt>
<dd>The minimal precision of a rule to be selected.</dd>
<dt><strong><code>recall_min</code></strong> :&ensp;<code>float</code>, optional (default=<code>0.01</code>)</dt>
<dd>The minimal recall of a rule to be selected.</dd>
<dt><strong><code>n_estimators</code></strong> :&ensp;<code>int</code>, optional (default=<code>10</code>)</dt>
<dd>The number of base estimators (rules) to use for prediction. More are
built before selection. All are available in the estimators_ attribute.</dd>
<dt><strong><code>max_samples</code></strong> :&ensp;<code>int</code> or <code>float</code>, optional (default=<code>.8</code>)</dt>
<dd>The number of samples to draw from X to train each decision tree, from
which rules are generated and selected.
- If int, then draw <code>max_samples</code> samples.
- If float, then draw <code>max_samples * X.shape[0]</code> samples.
If max_samples is larger than the number of samples provided,
all samples will be used for all trees (no sampling).</dd>
<dt><strong><code>max_samples_features</code></strong> :&ensp;<code>int</code> or <code>float</code>, optional (default=<code>1.0</code>)</dt>
<dd>The number of features to draw from X to train each decision tree, from
which rules are generated and selected.
- If int, then draw <code>max_features</code> features.
- If float, then draw <code>max_features * X.shape[1]</code> features.</dd>
<dt><strong><code>bootstrap</code></strong> :&ensp;<code>boolean</code>, optional (default=<code>False</code>)</dt>
<dd>Whether samples are drawn with replacement.</dd>
<dt><strong><code>bootstrap_features</code></strong> :&ensp;<code>boolean</code>, optional (default=<code>False</code>)</dt>
<dd>Whether features are drawn with replacement.</dd>
<dt><strong><code>max_depth</code></strong> :&ensp;<code>integer</code> or <code>List</code> or <code>None</code>, optional (default=<code>3</code>)</dt>
<dd>The maximum depth of the decision trees. If None, then nodes are
expanded until all leaves are pure or until all leaves contain less
than min_samples_split samples.
If an iterable is passed, you will train n_estimators
for each tree depth. It allows you to create and compare
rules of different length.</dd>
<dt><strong><code>max_depth_duplication</code></strong> :&ensp;<code>integer</code>, optional (default=<code>None</code>)</dt>
<dd>The maximum depth of the decision tree for rule deduplication,
if None then no deduplication occurs.</dd>
<dt><strong><code>max_features</code></strong> :&ensp;<code>int</code>, <code>float</code>, <code>string</code> or <code>None</code>, optional (default=<code>"auto"</code>)</dt>
<dd>
<p>The number of features considered (by each decision tree) when looking
for the best split:</p>
<ul>
<li>If int, then consider <code>max_features</code> features at each split.</li>
<li>If float, then <code>max_features</code> is a percentage and
<code>int(max_features * n_features)</code> features are considered at each
split.</li>
<li>If "auto", then <code>max_features=sqrt(n_features)</code>.</li>
<li>If "sqrt", then <code>max_features=sqrt(n_features)</code> (same as "auto").</li>
<li>If "log2", then <code>max_features=log2(n_features)</code>.</li>
<li>If None, then <code>max_features=n_features</code>.</li>
</ul>
<p>Note: the search for a split does not stop until at least one
valid partition of the node samples is found, even if it requires to
effectively inspect more than <code>max_features</code> features.</p>
</dd>
<dt><strong><code>min_samples_split</code></strong> :&ensp;<code>int</code>, <code>float</code>, optional (default=<code>2</code>)</dt>
<dd>The minimum number of samples required to split an internal node for
each decision tree.
- If int, then consider <code>min_samples_split</code> as the minimum number.
- If float, then <code>min_samples_split</code> is a percentage and
<code>ceil(min_samples_split * n_samples)</code> are the minimum
number of samples for each split.</dd>
<dt><strong><code>n_jobs</code></strong> :&ensp;<code>integer</code>, optional (default=<code>1</code>)</dt>
<dd>The number of jobs to run in parallel for both <code>fit</code> and <code>predict</code>.
If -1, then the number of jobs is set to the number of cores.</dd>
<dt><strong><code>random_state</code></strong> :&ensp;<code>int</code>, <code>RandomState</code> <code>instance</code> or <code>None</code>, optional</dt>
<dd>
<ul>
<li>If int, random_state is the seed used by the random number generator.</li>
<li>If RandomState instance, random_state is the random number generator.</li>
<li>If None, the random number generator is the RandomState instance used
by <code>np.random</code>.</li>
</ul>
</dd>
<dt><strong><code>verbose</code></strong> :&ensp;<code>int</code>, optional (default=<code>0</code>)</dt>
<dd>Controls the verbosity of the tree building process.</dd>
</dl>
<h2 id="attributes">Attributes</h2>
<p>rules_ : dict of tuples (rule, precision, recall, nb).
The collection of <code>n_estimators</code> rules used in the <code>predict</code> method.
The rules are generated by fitted sub-estimators (decision trees). Each
rule satisfies recall_min and precision_min conditions. The selection
is done according to OOB precisions.</p>
<dl>
<dt><strong><code>estimators_</code></strong> :&ensp;<code>list</code> of <code>DecisionTreeClassifier</code></dt>
<dd>The collection of fitted sub-estimators used to generate candidate
rules.</dd>
<dt><strong><code>estimators_samples_</code></strong> :&ensp;<code>list</code> of <code>arrays</code></dt>
<dd>The subset of drawn samples (i.e., the in-bag samples) for each base
estimator.</dd>
<dt><strong><code>estimators_features_</code></strong> :&ensp;<code>list</code> of <code>arrays</code></dt>
<dd>The subset of drawn features for each base estimator.</dd>
<dt><strong><code>max_samples_</code></strong> :&ensp;<code>integer</code></dt>
<dd>The actual number of samples</dd>
<dt><strong><code>n_features_</code></strong> :&ensp;<code>integer</code></dt>
<dd>The number of features when <code>fit</code> is performed.</dd>
<dt><strong><code>classes_</code></strong> :&ensp;<code>array</code>, <code>shape</code> (<code>n_classes</code>,)</dt>
<dd>The classes labels.</dd>
</dl></section>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class SkopeRules(BaseEstimator):
    &#34;&#34;&#34;An easy-interpretable classifier optimizing simple logical rules.

    Parameters
    ----------

    feature_names : list of str, optional
        The names of each feature to be used for returning rules in string
        format.

    precision_min : float, optional (default=0.5)
        The minimal precision of a rule to be selected.

    recall_min : float, optional (default=0.01)
        The minimal recall of a rule to be selected.

    n_estimators : int, optional (default=10)
        The number of base estimators (rules) to use for prediction. More are
        built before selection. All are available in the estimators_ attribute.

    max_samples : int or float, optional (default=.8)
        The number of samples to draw from X to train each decision tree, from
        which rules are generated and selected.
            - If int, then draw `max_samples` samples.
            - If float, then draw `max_samples * X.shape[0]` samples.
        If max_samples is larger than the number of samples provided,
        all samples will be used for all trees (no sampling).

    max_samples_features : int or float, optional (default=1.0)
        The number of features to draw from X to train each decision tree, from
        which rules are generated and selected.
            - If int, then draw `max_features` features.
            - If float, then draw `max_features * X.shape[1]` features.

    bootstrap : boolean, optional (default=False)
        Whether samples are drawn with replacement.

    bootstrap_features : boolean, optional (default=False)
        Whether features are drawn with replacement.

    max_depth : integer or List or None, optional (default=3)
        The maximum depth of the decision trees. If None, then nodes are
        expanded until all leaves are pure or until all leaves contain less
        than min_samples_split samples.
        If an iterable is passed, you will train n_estimators
        for each tree depth. It allows you to create and compare
        rules of different length.

    max_depth_duplication : integer, optional (default=None)
        The maximum depth of the decision tree for rule deduplication,
        if None then no deduplication occurs.

    max_features : int, float, string or None, optional (default=&#34;auto&#34;)
        The number of features considered (by each decision tree) when looking
        for the best split:

        - If int, then consider `max_features` features at each split.
        - If float, then `max_features` is a percentage and
          `int(max_features * n_features)` features are considered at each
          split.
        - If &#34;auto&#34;, then `max_features=sqrt(n_features)`.
        - If &#34;sqrt&#34;, then `max_features=sqrt(n_features)` (same as &#34;auto&#34;).
        - If &#34;log2&#34;, then `max_features=log2(n_features)`.
        - If None, then `max_features=n_features`.

        Note: the search for a split does not stop until at least one
        valid partition of the node samples is found, even if it requires to
        effectively inspect more than ``max_features`` features.

    min_samples_split : int, float, optional (default=2)
        The minimum number of samples required to split an internal node for
        each decision tree.
            - If int, then consider `min_samples_split` as the minimum number.
            - If float, then `min_samples_split` is a percentage and
              `ceil(min_samples_split * n_samples)` are the minimum
              number of samples for each split.

    n_jobs : integer, optional (default=1)
        The number of jobs to run in parallel for both `fit` and `predict`.
        If -1, then the number of jobs is set to the number of cores.

    random_state : int, RandomState instance or None, optional
        - If int, random_state is the seed used by the random number generator.
        - If RandomState instance, random_state is the random number generator.
        - If None, the random number generator is the RandomState instance used
        by `np.random`.

    verbose : int, optional (default=0)
        Controls the verbosity of the tree building process.

    Attributes
    ----------
    rules_ : dict of tuples (rule, precision, recall, nb).
        The collection of `n_estimators` rules used in the ``predict`` method.
        The rules are generated by fitted sub-estimators (decision trees). Each
        rule satisfies recall_min and precision_min conditions. The selection
        is done according to OOB precisions.

    estimators_ : list of DecisionTreeClassifier
        The collection of fitted sub-estimators used to generate candidate
        rules.

    estimators_samples_ : list of arrays
        The subset of drawn samples (i.e., the in-bag samples) for each base
        estimator.

    estimators_features_ : list of arrays
        The subset of drawn features for each base estimator.

    max_samples_ : integer
        The actual number of samples

    n_features_ : integer
        The number of features when ``fit`` is performed.

    classes_ : array, shape (n_classes,)
        The classes labels.
    &#34;&#34;&#34;

    def __init__(self,
                 feature_names=None,
                 precision_min=0.5,
                 recall_min=0.01,
                 n_estimators=10,
                 max_samples=.8,
                 max_samples_features=1.,
                 bootstrap=False,
                 bootstrap_features=False,
                 max_depth=3,
                 max_depth_duplication=None,
                 max_features=1.,
                 min_samples_split=2,
                 n_jobs=1,
                 random_state=None,
                 verbose=0):
        self.precision_min = precision_min
        self.recall_min = recall_min
        self.feature_names = feature_names
        self.n_estimators = n_estimators
        self.max_samples = max_samples
        self.max_samples_features = max_samples_features
        self.bootstrap = bootstrap
        self.bootstrap_features = bootstrap_features
        self.max_depth = max_depth
        self.max_depth_duplication = max_depth_duplication
        self.max_features = max_features
        self.min_samples_split = min_samples_split
        self.n_jobs = n_jobs
        self.random_state = random_state
        self.verbose = verbose

    def fit(self, X, y, sample_weight=None):
        &#34;&#34;&#34;Fit the model according to the given training data.

        Parameters
        ----------
        X : array-like, shape (n_samples, n_features)
            Training vector, where n_samples is the number of samples and
            n_features is the number of features.

        y : array-like, shape (n_samples,)
            Target vector relative to X. Has to follow the convention 0 for
            normal data, 1 for anomalies.

        sample_weight : array-like, shape (n_samples,) optional
            Array of weights that are assigned to individual samples, typically
            the amount in case of transactions data. Used to grow regression
            trees producing further rules to be tested.
            If not provided, then each sample is given unit weight.

        Returns
        -------
        self : object
            Returns self.
        &#34;&#34;&#34;

        X, y = check_X_y(X, y)
        check_classification_targets(y)
        self.n_features_ = X.shape[1]

        self.classes_ = np.unique(y)
        n_classes = len(self.classes_)

        if n_classes &lt; 2:
            raise ValueError(&#34;This method needs samples of at least 2 classes&#34;
                             &#34; in the data, but the data contains only one&#34;
                             &#34; class: %r&#34; % self.classes_[0])

        if not isinstance(self.max_depth_duplication, int) \
                and self.max_depth_duplication is not None:
            raise ValueError(&#34;max_depth_duplication should be an integer&#34;
                             )
        if not set(self.classes_) == set([0, 1]):
            warn(&#34;Found labels %s. This method assumes target class to be&#34;
                 &#34; labeled as 1 and normal data to be labeled as 0. Any label&#34;
                 &#34; different from 0 will be considered as being from the&#34;
                 &#34; target class.&#34;
                 % set(self.classes_))
            y = (y &gt; 0)

        # ensure that max_samples is in [1, n_samples]:
        n_samples = X.shape[0]

        if isinstance(self.max_samples, six.string_types):
            raise ValueError(&#39;max_samples (%s) is not supported.&#39;
                             &#39;Valid choices are: &#34;auto&#34;, int or&#39;
                             &#39;float&#39; % self.max_samples)

        elif isinstance(self.max_samples, INTEGER_TYPES):
            if self.max_samples &gt; n_samples:
                warn(&#34;max_samples (%s) is greater than the &#34;
                     &#34;total number of samples (%s). max_samples &#34;
                     &#34;will be set to n_samples for estimation.&#34;
                     % (self.max_samples, n_samples))
                max_samples = n_samples
            else:
                max_samples = self.max_samples
        else:  # float
            if not (0. &lt; self.max_samples &lt;= 1.):
                raise ValueError(&#34;max_samples must be in (0, 1], got %r&#34;
                                 % self.max_samples)
            max_samples = int(self.max_samples * X.shape[0])

        self.max_samples_ = max_samples

        self.rules_ = {}
        self.estimators_ = []
        self.estimators_samples_ = []
        self.estimators_features_ = []

        # default columns names :
        feature_names_ = [BASE_FEATURE_NAME + x for x in
                          np.arange(X.shape[1]).astype(str)]
        if self.feature_names is not None:
            self.feature_dict_ = {BASE_FEATURE_NAME + str(i): feat
                                  for i, feat in enumerate(self.feature_names)}
        else:
            self.feature_dict_ = {BASE_FEATURE_NAME + str(i): feat
                                  for i, feat in enumerate(feature_names_)}
        self.feature_names_ = feature_names_

        clfs = []
        regs = []

        self._max_depths = self.max_depth \
            if isinstance(self.max_depth, Iterable) else [self.max_depth]

        for max_depth in self._max_depths:
            bagging_clf = BaggingClassifier(
                base_estimator=DecisionTreeClassifier(
                    max_depth=max_depth,
                    max_features=self.max_features,
                    min_samples_split=self.min_samples_split),
                n_estimators=self.n_estimators,
                max_samples=self.max_samples_,
                max_features=self.max_samples_features,
                bootstrap=self.bootstrap,
                bootstrap_features=self.bootstrap_features,
                # oob_score=... XXX may be added
                # if selection on tree perf needed.
                # warm_start=... XXX may be added to increase computation perf.
                n_jobs=self.n_jobs,
                random_state=self.random_state,
                verbose=self.verbose)

            bagging_reg = BaggingRegressor(
                base_estimator=DecisionTreeRegressor(
                    max_depth=max_depth,
                    max_features=self.max_features,
                    min_samples_split=self.min_samples_split),
                n_estimators=self.n_estimators,
                max_samples=self.max_samples_,
                max_features=self.max_samples_features,
                bootstrap=self.bootstrap,
                bootstrap_features=self.bootstrap_features,
                # oob_score=... XXX may be added
                # if selection on tree perf needed.
                # warm_start=... XXX may be added to increase computation perf.
                n_jobs=self.n_jobs,
                random_state=self.random_state,
                verbose=self.verbose)

            clfs.append(bagging_clf)
            regs.append(bagging_reg)

        # define regression target:
        if sample_weight is not None:
            if sample_weight is not None:
                sample_weight = check_array(sample_weight, ensure_2d=False)
            weights = sample_weight - sample_weight.min()
            contamination = float(sum(y)) / len(y)
            y_reg = (
                pow(weights, 0.5) * 0.5 / contamination * (y &gt; 0) -
                pow((weights).mean(), 0.5) * (y == 0))
            y_reg = 1. / (1 + np.exp(-y_reg))  # sigmoid
        else:
            y_reg = y  # same as an other classification bagging

        for clf in clfs:
            clf.fit(X, y)
            self.estimators_ += clf.estimators_
            self.estimators_samples_ += clf.estimators_samples_
            self.estimators_features_ += clf.estimators_features_

        for reg in regs:
            reg.fit(X, y_reg)
            self.estimators_ += reg.estimators_
            self.estimators_samples_ += reg.estimators_samples_
            self.estimators_features_ += reg.estimators_features_

        rules_ = []
        for estimator, samples, features in zip(self.estimators_,
                                                self.estimators_samples_,
                                                self.estimators_features_):

            # Create mask for OOB samples
            mask = ~samples
            if sum(mask) == 0:
                warn(&#34;OOB evaluation not possible: doing it in-bag.&#34;
                     &#34; Performance evaluation is likely to be wrong&#34;
                     &#34; (overfitting) and selected rules are likely to&#34;
                     &#34; not perform well! Please use max_samples &lt; 1.&#34;)
                mask = samples
            rules_from_tree = self._tree_to_rules(
                estimator, np.array(self.feature_names_)[features])

            # XXX todo: idem without dataframe
            X_oob = pandas.DataFrame((X[mask, :])[:, features],
                                     columns=np.array(
                                         self.feature_names_)[features])

            if X_oob.shape[1] &gt; 1:  # otherwise pandas bug (cf. issue #16363)
                y_oob = y[mask]
                y_oob = np.array((y_oob != 0))

                # Add OOB performances to rules:
                rules_from_tree = [(r, self._eval_rule_perf(r, X_oob, y_oob))
                                   for r in set(rules_from_tree)]
                rules_ += rules_from_tree

        # Factorize rules before semantic tree filtering
        rules_ = [
            tuple(rule)
            for rule in
            [Rule(r, args=args) for r, args in rules_]]

        # keep only rules verifying precision_min and recall_min:
        for rule, score in rules_:
            if score[0] &gt;= self.precision_min and score[1] &gt;= self.recall_min:
                if rule in self.rules_:
                    # update the score to the new mean
                    c = self.rules_[rule][2] + 1
                    b = self.rules_[rule][1] + 1. / c * (
                        score[1] - self.rules_[rule][1])
                    a = self.rules_[rule][0] + 1. / c * (
                        score[0] - self.rules_[rule][0])

                    self.rules_[rule] = (a, b, c)
                else:
                    self.rules_[rule] = (score[0], score[1], 1)

        self.rules_ = sorted(self.rules_.items(),
                             key=lambda x: (x[1][0], x[1][1]), reverse=True)

        # Deduplicate the rule using semantic tree
        if self.max_depth_duplication is not None:
            self.rules_ = self.deduplicate(self.rules_)

        self.rules_ = sorted(self.rules_, key=lambda x: - self.f1_score(x))
        self.rules_without_feature_names_ = self.rules_

        # Replace generic feature names by real feature names
        self.rules_ = [(replace_feature_name(rule, self.feature_dict_), perf)
                       for rule, perf in self.rules_]

        return self

    def predict(self, X):
        &#34;&#34;&#34;Predict if a particular sample is an outlier or not.

        Parameters
        ----------
        X : array-like, shape (n_samples, n_features)
            The input samples. Internally, it will be converted to
            ``dtype=np.float32``

        Returns
        -------
        is_outlier : array, shape (n_samples,)
            For each observations, tells whether or not (1 or 0) it should
            be considered as an outlier according to the selected rules.
        &#34;&#34;&#34;

        return np.array((self.decision_function(X) &gt; 0), dtype=int)

    def decision_function(self, X):
        &#34;&#34;&#34;Average anomaly score of X of the base classifiers (rules).

        The anomaly score of an input sample is computed as
        the weighted sum of the binary rules outputs, the weight being
        the respective precision of each rule.

        Parameters
        ----------
        X : array-like, shape (n_samples, n_features)
            The training input samples.

        Returns
        -------
        scores : array, shape (n_samples,)
            The anomaly score of the input samples.
            The higher, the more abnormal. Positive scores represent outliers,
            null scores represent inliers.

        &#34;&#34;&#34;
        # Check if fit had been called
        check_is_fitted(self, [&#39;rules_&#39;, &#39;estimators_&#39;, &#39;estimators_samples_&#39;,
                               &#39;max_samples_&#39;])

        # Input validation
        X = check_array(X)

        if X.shape[1] != self.n_features_:
            raise ValueError(&#34;X.shape[1] = %d should be equal to %d, &#34;
                             &#34;the number of features at training time.&#34;
                             &#34; Please reshape your data.&#34;
                             % (X.shape[1], self.n_features_))

        df = pandas.DataFrame(X, columns=self.feature_names_)
        selected_rules = self.rules_without_feature_names_

        scores = np.zeros(X.shape[0])
        for (r, w) in selected_rules:
            scores[list(df.query(r).index)] += w[0]

        return scores

    def rules_vote(self, X):
        &#34;&#34;&#34;Score representing a vote of the base classifiers (rules).

        The score of an input sample is computed as the sum of the binary
        rules outputs: a score of k means than k rules have voted positively.

        Parameters
        ----------
        X : array-like, shape (n_samples, n_features)
            The training input samples.

        Returns
        -------
        scores : array, shape (n_samples,)
            The score of the input samples.
            The higher, the more abnormal. Positive scores represent outliers,
            null scores represent inliers.

        &#34;&#34;&#34;
        # Check if fit had been called
        check_is_fitted(self, [&#39;rules_&#39;, &#39;estimators_&#39;, &#39;estimators_samples_&#39;,
                               &#39;max_samples_&#39;])

        # Input validation
        X = check_array(X)

        if X.shape[1] != self.n_features_:
            raise ValueError(&#34;X.shape[1] = %d should be equal to %d, &#34;
                             &#34;the number of features at training time.&#34;
                             &#34; Please reshape your data.&#34;
                             % (X.shape[1], self.n_features_))

        df = pandas.DataFrame(X, columns=self.feature_names_)
        selected_rules = self.rules_

        scores = np.zeros(X.shape[0])
        for (r, _) in selected_rules:
            scores[list(df.query(r).index)] += 1

        return scores

    def score_top_rules(self, X):
        &#34;&#34;&#34;Score representing an ordering between the base classifiers (rules).

        The score is high when the instance is detected by a performing rule.
        If there are n rules, ordered by increasing OOB precision, a score of k
        means than the kth rule has voted positively, but not the (k-1) first
        rules.

        Parameters
        ----------
        X : array-like, shape (n_samples, n_features)
            The training input samples.

        Returns
        -------
        scores : array, shape (n_samples,)
            The score of the input samples.
            Positive scores represent outliers, null scores represent inliers.

        &#34;&#34;&#34;
        # Check if fit had been called
        check_is_fitted(self, [&#39;rules_&#39;, &#39;estimators_&#39;, &#39;estimators_samples_&#39;,
                               &#39;max_samples_&#39;])

        # Input validation
        X = check_array(X)

        if X.shape[1] != self.n_features_:
            raise ValueError(&#34;X.shape[1] = %d should be equal to %d, &#34;
                             &#34;the number of features at training time.&#34;
                             &#34; Please reshape your data.&#34;
                             % (X.shape[1], self.n_features_))

        df = pandas.DataFrame(X, columns=self.feature_names_)
        selected_rules = self.rules_without_feature_names_

        scores = np.zeros(X.shape[0])
        for (k, r) in enumerate(list((selected_rules))):
            scores[list(df.query(r[0]).index)] = np.maximum(
                len(selected_rules) - k,
                scores[list(df.query(r[0]).index)])

        return scores
    
    def predict_proba(self, X):
        y = self.score_top_rules(X)
        return np.vstack((1 - y, y)).transpose()

    def predict_top_rules(self, X, n_rules):
        &#34;&#34;&#34;Predict if a particular sample is an outlier or not,
        using the n_rules most performing rules.

        Parameters
        ----------
        X : array-like, shape (n_samples, n_features)
            The input samples. Internally, it will be converted to
            ``dtype=np.float32``

        n_rules : int
            The number of rules used for the prediction. If one of the
            n_rules most performing rules is activated, the prediction
            is equal to 1.

        Returns
        -------
        is_outlier : array, shape (n_samples,)
            For each observations, tells whether or not (1 or 0) it should
            be considered as an outlier according to the selected rules.
        &#34;&#34;&#34;

        return np.array((self.score_top_rules(X) &gt; len(self.rules_) - n_rules),
                        dtype=int)

    def _tree_to_rules(self, tree, feature_names):
        &#34;&#34;&#34;
        Return a list of rules from a tree

        Parameters
        ----------
            tree : Decision Tree Classifier/Regressor
            feature_names: list of variable names

        Returns
        -------
        rules : list of rules.
        &#34;&#34;&#34;
        # XXX todo: check the case where tree is build on subset of features,
        # ie max_features != None

        tree_ = tree.tree_
        feature_name = [
            feature_names[i] if i != _tree.TREE_UNDEFINED else &#34;undefined!&#34;
            for i in tree_.feature
        ]
        rules = []

        def recurse(node, base_name):
            if tree_.feature[node] != _tree.TREE_UNDEFINED:
                name = feature_name[node]
                symbol = &#39;&lt;=&#39;
                symbol2 = &#39;&gt;&#39;
                threshold = tree_.threshold[node]
                text = base_name + [&#34;{} {} {}&#34;.format(name, symbol, threshold)]
                recurse(tree_.children_left[node], text)

                text = base_name + [&#34;{} {} {}&#34;.format(name, symbol2,
                                                      threshold)]
                recurse(tree_.children_right[node], text)
            else:
                rule = str.join(&#39; and &#39;, base_name)
                rule = (rule if rule != &#39;&#39;
                        else &#39; == &#39;.join([feature_names[0]] * 2))
                # a rule selecting all is set to &#34;c0==c0&#34;
                rules.append(rule)

        recurse(0, [])

        return rules if len(rules) &gt; 0 else &#39;True&#39;

    def _eval_rule_perf(self, rule, X, y):
        detected_index = list(X.query(rule).index)
        if len(detected_index) &lt;= 1:
            return (0, 0)
        y_detected = y[detected_index]
        true_pos = y_detected[y_detected &gt; 0].sum()
        if true_pos == 0:
            return (0, 0)
        pos = y[y &gt; 0].sum()
        return y_detected.mean(), float(true_pos) / pos

    def deduplicate(self, rules):
        return [max(rules_set, key=self.f1_score)
                for rules_set in self._find_similar_rulesets(rules)]

    def _find_similar_rulesets(self, rules):
        &#34;&#34;&#34;Create clusters of rules using a decision tree based
        on the terms of the rules

        Parameters
        ----------
        rules : List, List of rules
                The rules that should be splitted in subsets of similar rules

        Returns
        -------
        rules : List of list of rules
                The different set of rules. Each set should be homogeneous

        &#34;&#34;&#34;
        def split_with_best_feature(rules, depth, exceptions=[]):
            &#34;&#34;&#34;
            Method to find a split of rules given most represented feature
            &#34;&#34;&#34;
            if depth == 0:
                return rules

            rulelist = [rule.split(&#39; and &#39;) for rule, score in rules]
            terms = [t.split(&#39; &#39;)[0] for term in rulelist for t in term]
            counter = Counter(terms)
            # Drop exception list
            for exception in exceptions:
                del counter[exception]

            if len(counter) == 0:
                return rules

            most_represented_term = counter.most_common()[0][0]
            # Proceed to split
            rules_splitted = [[], [], []]
            for rule in rules:
                if (most_represented_term + &#39; &lt;=&#39;) in rule[0]:
                    rules_splitted[0].append(rule)
                elif (most_represented_term + &#39; &gt;&#39;) in rule[0]:
                    rules_splitted[1].append(rule)
                else:
                    rules_splitted[2].append(rule)
            new_exceptions = exceptions+[most_represented_term]
            # Choose best term
            return [split_with_best_feature(ruleset,
                                            depth-1,
                                            exceptions=new_exceptions)
                    for ruleset in rules_splitted]

        def breadth_first_search(rules, leaves=None):
            if len(rules) == 0 or not isinstance(rules[0], list):
                if len(rules) &gt; 0:
                    return leaves.append(rules)
            else:
                for rules_child in rules:
                    breadth_first_search(rules_child, leaves=leaves)
            return leaves
        leaves = []
        res = split_with_best_feature(rules, self.max_depth_duplication)
        breadth_first_search(res, leaves=leaves)
        return leaves

    def f1_score(self, x):
        return 2 * x[1][0] * x[1][1] / \
               (x[1][0] + x[1][1]) if (x[1][0] + x[1][1]) &gt; 0 else 0</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li><a title="sklearn.base.BaseEstimator" href="/sklearn.base.BaseEstimator.ext">sklearn.base.BaseEstimator</a></li>
</ul>
<h3>Methods</h3>
<dl>
<dt id="imodels.skrules.SkopeRules.decision_function"><code class="name flex">
<span>def <span class="ident">decision_function</span></span>(<span>self, X)</span>
</code></dt>
<dd>
<section class="desc"><p>Average anomaly score of X of the base classifiers (rules).</p>
<p>The anomaly score of an input sample is computed as
the weighted sum of the binary rules outputs, the weight being
the respective precision of each rule.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>X</code></strong> :&ensp;<code>array</code>-<code>like</code>, <code>shape</code> (<code>n_samples</code>, <code>n_features</code>)</dt>
<dd>The training input samples.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>scores</code></strong> :&ensp;<code>array</code>, <code>shape</code> (<code>n_samples</code>,)</dt>
<dd>The anomaly score of the input samples.
The higher, the more abnormal. Positive scores represent outliers,
null scores represent inliers.</dd>
</dl></section>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def decision_function(self, X):
    &#34;&#34;&#34;Average anomaly score of X of the base classifiers (rules).

    The anomaly score of an input sample is computed as
    the weighted sum of the binary rules outputs, the weight being
    the respective precision of each rule.

    Parameters
    ----------
    X : array-like, shape (n_samples, n_features)
        The training input samples.

    Returns
    -------
    scores : array, shape (n_samples,)
        The anomaly score of the input samples.
        The higher, the more abnormal. Positive scores represent outliers,
        null scores represent inliers.

    &#34;&#34;&#34;
    # Check if fit had been called
    check_is_fitted(self, [&#39;rules_&#39;, &#39;estimators_&#39;, &#39;estimators_samples_&#39;,
                           &#39;max_samples_&#39;])

    # Input validation
    X = check_array(X)

    if X.shape[1] != self.n_features_:
        raise ValueError(&#34;X.shape[1] = %d should be equal to %d, &#34;
                         &#34;the number of features at training time.&#34;
                         &#34; Please reshape your data.&#34;
                         % (X.shape[1], self.n_features_))

    df = pandas.DataFrame(X, columns=self.feature_names_)
    selected_rules = self.rules_without_feature_names_

    scores = np.zeros(X.shape[0])
    for (r, w) in selected_rules:
        scores[list(df.query(r).index)] += w[0]

    return scores</code></pre>
</details>
</dd>
<dt id="imodels.skrules.SkopeRules.deduplicate"><code class="name flex">
<span>def <span class="ident">deduplicate</span></span>(<span>self, rules)</span>
</code></dt>
<dd>
<section class="desc"></section>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def deduplicate(self, rules):
    return [max(rules_set, key=self.f1_score)
            for rules_set in self._find_similar_rulesets(rules)]</code></pre>
</details>
</dd>
<dt id="imodels.skrules.SkopeRules.f1_score"><code class="name flex">
<span>def <span class="ident">f1_score</span></span>(<span>self, x)</span>
</code></dt>
<dd>
<section class="desc"></section>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def f1_score(self, x):
    return 2 * x[1][0] * x[1][1] / \
           (x[1][0] + x[1][1]) if (x[1][0] + x[1][1]) &gt; 0 else 0</code></pre>
</details>
</dd>
<dt id="imodels.skrules.SkopeRules.fit"><code class="name flex">
<span>def <span class="ident">fit</span></span>(<span>self, X, y, sample_weight=None)</span>
</code></dt>
<dd>
<section class="desc"><p>Fit the model according to the given training data.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>X</code></strong> :&ensp;<code>array</code>-<code>like</code>, <code>shape</code> (<code>n_samples</code>, <code>n_features</code>)</dt>
<dd>Training vector, where n_samples is the number of samples and
n_features is the number of features.</dd>
<dt><strong><code>y</code></strong> :&ensp;<code>array</code>-<code>like</code>, <code>shape</code> (<code>n_samples</code>,)</dt>
<dd>Target vector relative to X. Has to follow the convention 0 for
normal data, 1 for anomalies.</dd>
<dt><strong><code>sample_weight</code></strong> :&ensp;<code>array</code>-<code>like</code>, <code>shape</code> (<code>n_samples</code>,) optional</dt>
<dd>Array of weights that are assigned to individual samples, typically
the amount in case of transactions data. Used to grow regression
trees producing further rules to be tested.
If not provided, then each sample is given unit weight.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>self</code></strong> :&ensp;<code>object</code></dt>
<dd>Returns self.</dd>
</dl></section>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def fit(self, X, y, sample_weight=None):
    &#34;&#34;&#34;Fit the model according to the given training data.

    Parameters
    ----------
    X : array-like, shape (n_samples, n_features)
        Training vector, where n_samples is the number of samples and
        n_features is the number of features.

    y : array-like, shape (n_samples,)
        Target vector relative to X. Has to follow the convention 0 for
        normal data, 1 for anomalies.

    sample_weight : array-like, shape (n_samples,) optional
        Array of weights that are assigned to individual samples, typically
        the amount in case of transactions data. Used to grow regression
        trees producing further rules to be tested.
        If not provided, then each sample is given unit weight.

    Returns
    -------
    self : object
        Returns self.
    &#34;&#34;&#34;

    X, y = check_X_y(X, y)
    check_classification_targets(y)
    self.n_features_ = X.shape[1]

    self.classes_ = np.unique(y)
    n_classes = len(self.classes_)

    if n_classes &lt; 2:
        raise ValueError(&#34;This method needs samples of at least 2 classes&#34;
                         &#34; in the data, but the data contains only one&#34;
                         &#34; class: %r&#34; % self.classes_[0])

    if not isinstance(self.max_depth_duplication, int) \
            and self.max_depth_duplication is not None:
        raise ValueError(&#34;max_depth_duplication should be an integer&#34;
                         )
    if not set(self.classes_) == set([0, 1]):
        warn(&#34;Found labels %s. This method assumes target class to be&#34;
             &#34; labeled as 1 and normal data to be labeled as 0. Any label&#34;
             &#34; different from 0 will be considered as being from the&#34;
             &#34; target class.&#34;
             % set(self.classes_))
        y = (y &gt; 0)

    # ensure that max_samples is in [1, n_samples]:
    n_samples = X.shape[0]

    if isinstance(self.max_samples, six.string_types):
        raise ValueError(&#39;max_samples (%s) is not supported.&#39;
                         &#39;Valid choices are: &#34;auto&#34;, int or&#39;
                         &#39;float&#39; % self.max_samples)

    elif isinstance(self.max_samples, INTEGER_TYPES):
        if self.max_samples &gt; n_samples:
            warn(&#34;max_samples (%s) is greater than the &#34;
                 &#34;total number of samples (%s). max_samples &#34;
                 &#34;will be set to n_samples for estimation.&#34;
                 % (self.max_samples, n_samples))
            max_samples = n_samples
        else:
            max_samples = self.max_samples
    else:  # float
        if not (0. &lt; self.max_samples &lt;= 1.):
            raise ValueError(&#34;max_samples must be in (0, 1], got %r&#34;
                             % self.max_samples)
        max_samples = int(self.max_samples * X.shape[0])

    self.max_samples_ = max_samples

    self.rules_ = {}
    self.estimators_ = []
    self.estimators_samples_ = []
    self.estimators_features_ = []

    # default columns names :
    feature_names_ = [BASE_FEATURE_NAME + x for x in
                      np.arange(X.shape[1]).astype(str)]
    if self.feature_names is not None:
        self.feature_dict_ = {BASE_FEATURE_NAME + str(i): feat
                              for i, feat in enumerate(self.feature_names)}
    else:
        self.feature_dict_ = {BASE_FEATURE_NAME + str(i): feat
                              for i, feat in enumerate(feature_names_)}
    self.feature_names_ = feature_names_

    clfs = []
    regs = []

    self._max_depths = self.max_depth \
        if isinstance(self.max_depth, Iterable) else [self.max_depth]

    for max_depth in self._max_depths:
        bagging_clf = BaggingClassifier(
            base_estimator=DecisionTreeClassifier(
                max_depth=max_depth,
                max_features=self.max_features,
                min_samples_split=self.min_samples_split),
            n_estimators=self.n_estimators,
            max_samples=self.max_samples_,
            max_features=self.max_samples_features,
            bootstrap=self.bootstrap,
            bootstrap_features=self.bootstrap_features,
            # oob_score=... XXX may be added
            # if selection on tree perf needed.
            # warm_start=... XXX may be added to increase computation perf.
            n_jobs=self.n_jobs,
            random_state=self.random_state,
            verbose=self.verbose)

        bagging_reg = BaggingRegressor(
            base_estimator=DecisionTreeRegressor(
                max_depth=max_depth,
                max_features=self.max_features,
                min_samples_split=self.min_samples_split),
            n_estimators=self.n_estimators,
            max_samples=self.max_samples_,
            max_features=self.max_samples_features,
            bootstrap=self.bootstrap,
            bootstrap_features=self.bootstrap_features,
            # oob_score=... XXX may be added
            # if selection on tree perf needed.
            # warm_start=... XXX may be added to increase computation perf.
            n_jobs=self.n_jobs,
            random_state=self.random_state,
            verbose=self.verbose)

        clfs.append(bagging_clf)
        regs.append(bagging_reg)

    # define regression target:
    if sample_weight is not None:
        if sample_weight is not None:
            sample_weight = check_array(sample_weight, ensure_2d=False)
        weights = sample_weight - sample_weight.min()
        contamination = float(sum(y)) / len(y)
        y_reg = (
            pow(weights, 0.5) * 0.5 / contamination * (y &gt; 0) -
            pow((weights).mean(), 0.5) * (y == 0))
        y_reg = 1. / (1 + np.exp(-y_reg))  # sigmoid
    else:
        y_reg = y  # same as an other classification bagging

    for clf in clfs:
        clf.fit(X, y)
        self.estimators_ += clf.estimators_
        self.estimators_samples_ += clf.estimators_samples_
        self.estimators_features_ += clf.estimators_features_

    for reg in regs:
        reg.fit(X, y_reg)
        self.estimators_ += reg.estimators_
        self.estimators_samples_ += reg.estimators_samples_
        self.estimators_features_ += reg.estimators_features_

    rules_ = []
    for estimator, samples, features in zip(self.estimators_,
                                            self.estimators_samples_,
                                            self.estimators_features_):

        # Create mask for OOB samples
        mask = ~samples
        if sum(mask) == 0:
            warn(&#34;OOB evaluation not possible: doing it in-bag.&#34;
                 &#34; Performance evaluation is likely to be wrong&#34;
                 &#34; (overfitting) and selected rules are likely to&#34;
                 &#34; not perform well! Please use max_samples &lt; 1.&#34;)
            mask = samples
        rules_from_tree = self._tree_to_rules(
            estimator, np.array(self.feature_names_)[features])

        # XXX todo: idem without dataframe
        X_oob = pandas.DataFrame((X[mask, :])[:, features],
                                 columns=np.array(
                                     self.feature_names_)[features])

        if X_oob.shape[1] &gt; 1:  # otherwise pandas bug (cf. issue #16363)
            y_oob = y[mask]
            y_oob = np.array((y_oob != 0))

            # Add OOB performances to rules:
            rules_from_tree = [(r, self._eval_rule_perf(r, X_oob, y_oob))
                               for r in set(rules_from_tree)]
            rules_ += rules_from_tree

    # Factorize rules before semantic tree filtering
    rules_ = [
        tuple(rule)
        for rule in
        [Rule(r, args=args) for r, args in rules_]]

    # keep only rules verifying precision_min and recall_min:
    for rule, score in rules_:
        if score[0] &gt;= self.precision_min and score[1] &gt;= self.recall_min:
            if rule in self.rules_:
                # update the score to the new mean
                c = self.rules_[rule][2] + 1
                b = self.rules_[rule][1] + 1. / c * (
                    score[1] - self.rules_[rule][1])
                a = self.rules_[rule][0] + 1. / c * (
                    score[0] - self.rules_[rule][0])

                self.rules_[rule] = (a, b, c)
            else:
                self.rules_[rule] = (score[0], score[1], 1)

    self.rules_ = sorted(self.rules_.items(),
                         key=lambda x: (x[1][0], x[1][1]), reverse=True)

    # Deduplicate the rule using semantic tree
    if self.max_depth_duplication is not None:
        self.rules_ = self.deduplicate(self.rules_)

    self.rules_ = sorted(self.rules_, key=lambda x: - self.f1_score(x))
    self.rules_without_feature_names_ = self.rules_

    # Replace generic feature names by real feature names
    self.rules_ = [(replace_feature_name(rule, self.feature_dict_), perf)
                   for rule, perf in self.rules_]

    return self</code></pre>
</details>
</dd>
<dt id="imodels.skrules.SkopeRules.predict"><code class="name flex">
<span>def <span class="ident">predict</span></span>(<span>self, X)</span>
</code></dt>
<dd>
<section class="desc"><p>Predict if a particular sample is an outlier or not.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>X</code></strong> :&ensp;<code>array</code>-<code>like</code>, <code>shape</code> (<code>n_samples</code>, <code>n_features</code>)</dt>
<dd>The input samples. Internally, it will be converted to
<code>dtype=np.float32</code></dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>is_outlier</code></strong> :&ensp;<code>array</code>, <code>shape</code> (<code>n_samples</code>,)</dt>
<dd>For each observations, tells whether or not (1 or 0) it should
be considered as an outlier according to the selected rules.</dd>
</dl></section>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def predict(self, X):
    &#34;&#34;&#34;Predict if a particular sample is an outlier or not.

    Parameters
    ----------
    X : array-like, shape (n_samples, n_features)
        The input samples. Internally, it will be converted to
        ``dtype=np.float32``

    Returns
    -------
    is_outlier : array, shape (n_samples,)
        For each observations, tells whether or not (1 or 0) it should
        be considered as an outlier according to the selected rules.
    &#34;&#34;&#34;

    return np.array((self.decision_function(X) &gt; 0), dtype=int)</code></pre>
</details>
</dd>
<dt id="imodels.skrules.SkopeRules.predict_proba"><code class="name flex">
<span>def <span class="ident">predict_proba</span></span>(<span>self, X)</span>
</code></dt>
<dd>
<section class="desc"></section>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def predict_proba(self, X):
    y = self.score_top_rules(X)
    return np.vstack((1 - y, y)).transpose()</code></pre>
</details>
</dd>
<dt id="imodels.skrules.SkopeRules.predict_top_rules"><code class="name flex">
<span>def <span class="ident">predict_top_rules</span></span>(<span>self, X, n_rules)</span>
</code></dt>
<dd>
<section class="desc"><p>Predict if a particular sample is an outlier or not,
using the n_rules most performing rules.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>X</code></strong> :&ensp;<code>array</code>-<code>like</code>, <code>shape</code> (<code>n_samples</code>, <code>n_features</code>)</dt>
<dd>The input samples. Internally, it will be converted to
<code>dtype=np.float32</code></dd>
<dt><strong><code>n_rules</code></strong> :&ensp;<code>int</code></dt>
<dd>The number of rules used for the prediction. If one of the
n_rules most performing rules is activated, the prediction
is equal to 1.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>is_outlier</code></strong> :&ensp;<code>array</code>, <code>shape</code> (<code>n_samples</code>,)</dt>
<dd>For each observations, tells whether or not (1 or 0) it should
be considered as an outlier according to the selected rules.</dd>
</dl></section>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def predict_top_rules(self, X, n_rules):
    &#34;&#34;&#34;Predict if a particular sample is an outlier or not,
    using the n_rules most performing rules.

    Parameters
    ----------
    X : array-like, shape (n_samples, n_features)
        The input samples. Internally, it will be converted to
        ``dtype=np.float32``

    n_rules : int
        The number of rules used for the prediction. If one of the
        n_rules most performing rules is activated, the prediction
        is equal to 1.

    Returns
    -------
    is_outlier : array, shape (n_samples,)
        For each observations, tells whether or not (1 or 0) it should
        be considered as an outlier according to the selected rules.
    &#34;&#34;&#34;

    return np.array((self.score_top_rules(X) &gt; len(self.rules_) - n_rules),
                    dtype=int)</code></pre>
</details>
</dd>
<dt id="imodels.skrules.SkopeRules.rules_vote"><code class="name flex">
<span>def <span class="ident">rules_vote</span></span>(<span>self, X)</span>
</code></dt>
<dd>
<section class="desc"><p>Score representing a vote of the base classifiers (rules).</p>
<p>The score of an input sample is computed as the sum of the binary
rules outputs: a score of k means than k rules have voted positively.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>X</code></strong> :&ensp;<code>array</code>-<code>like</code>, <code>shape</code> (<code>n_samples</code>, <code>n_features</code>)</dt>
<dd>The training input samples.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>scores</code></strong> :&ensp;<code>array</code>, <code>shape</code> (<code>n_samples</code>,)</dt>
<dd>The score of the input samples.
The higher, the more abnormal. Positive scores represent outliers,
null scores represent inliers.</dd>
</dl></section>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def rules_vote(self, X):
    &#34;&#34;&#34;Score representing a vote of the base classifiers (rules).

    The score of an input sample is computed as the sum of the binary
    rules outputs: a score of k means than k rules have voted positively.

    Parameters
    ----------
    X : array-like, shape (n_samples, n_features)
        The training input samples.

    Returns
    -------
    scores : array, shape (n_samples,)
        The score of the input samples.
        The higher, the more abnormal. Positive scores represent outliers,
        null scores represent inliers.

    &#34;&#34;&#34;
    # Check if fit had been called
    check_is_fitted(self, [&#39;rules_&#39;, &#39;estimators_&#39;, &#39;estimators_samples_&#39;,
                           &#39;max_samples_&#39;])

    # Input validation
    X = check_array(X)

    if X.shape[1] != self.n_features_:
        raise ValueError(&#34;X.shape[1] = %d should be equal to %d, &#34;
                         &#34;the number of features at training time.&#34;
                         &#34; Please reshape your data.&#34;
                         % (X.shape[1], self.n_features_))

    df = pandas.DataFrame(X, columns=self.feature_names_)
    selected_rules = self.rules_

    scores = np.zeros(X.shape[0])
    for (r, _) in selected_rules:
        scores[list(df.query(r).index)] += 1

    return scores</code></pre>
</details>
</dd>
<dt id="imodels.skrules.SkopeRules.score_top_rules"><code class="name flex">
<span>def <span class="ident">score_top_rules</span></span>(<span>self, X)</span>
</code></dt>
<dd>
<section class="desc"><p>Score representing an ordering between the base classifiers (rules).</p>
<p>The score is high when the instance is detected by a performing rule.
If there are n rules, ordered by increasing OOB precision, a score of k
means than the kth rule has voted positively, but not the (k-1) first
rules.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>X</code></strong> :&ensp;<code>array</code>-<code>like</code>, <code>shape</code> (<code>n_samples</code>, <code>n_features</code>)</dt>
<dd>The training input samples.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>scores</code></strong> :&ensp;<code>array</code>, <code>shape</code> (<code>n_samples</code>,)</dt>
<dd>The score of the input samples.
Positive scores represent outliers, null scores represent inliers.</dd>
</dl></section>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def score_top_rules(self, X):
    &#34;&#34;&#34;Score representing an ordering between the base classifiers (rules).

    The score is high when the instance is detected by a performing rule.
    If there are n rules, ordered by increasing OOB precision, a score of k
    means than the kth rule has voted positively, but not the (k-1) first
    rules.

    Parameters
    ----------
    X : array-like, shape (n_samples, n_features)
        The training input samples.

    Returns
    -------
    scores : array, shape (n_samples,)
        The score of the input samples.
        Positive scores represent outliers, null scores represent inliers.

    &#34;&#34;&#34;
    # Check if fit had been called
    check_is_fitted(self, [&#39;rules_&#39;, &#39;estimators_&#39;, &#39;estimators_samples_&#39;,
                           &#39;max_samples_&#39;])

    # Input validation
    X = check_array(X)

    if X.shape[1] != self.n_features_:
        raise ValueError(&#34;X.shape[1] = %d should be equal to %d, &#34;
                         &#34;the number of features at training time.&#34;
                         &#34; Please reshape your data.&#34;
                         % (X.shape[1], self.n_features_))

    df = pandas.DataFrame(X, columns=self.feature_names_)
    selected_rules = self.rules_without_feature_names_

    scores = np.zeros(X.shape[0])
    for (k, r) in enumerate(list((selected_rules))):
        scores[list(df.query(r[0]).index)] = np.maximum(
            len(selected_rules) - k,
            scores[list(df.query(r[0]).index)])

    return scores</code></pre>
</details>
</dd>
</dl>
</dd>
</dl>
</section>
</article>
<nav id="sidebar">
<h1>Index</h1>
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="imodels" href="../index.html">imodels</a></code></li>
</ul>
</li>
<li><h3><a href="#header-submodules">Sub-modules</a></h3>
<ul>
<li><code><a title="imodels.skrules.datasets" href="datasets/index.html">imodels.skrules.datasets</a></code></li>
<li><code><a title="imodels.skrules.rule" href="rule.html">imodels.skrules.rule</a></code></li>
<li><code><a title="imodels.skrules.skope_rules" href="skope_rules.html">imodels.skrules.skope_rules</a></code></li>
<li><code><a title="imodels.skrules.tests" href="tests/index.html">imodels.skrules.tests</a></code></li>
</ul>
</li>
<li><h3><a href="#header-classes">Classes</a></h3>
<ul>
<li>
<h4><code><a title="imodels.skrules.Rule" href="#imodels.skrules.Rule">Rule</a></code></h4>
<ul class="">
<li><code><a title="imodels.skrules.Rule.factorize" href="#imodels.skrules.Rule.factorize">factorize</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="imodels.skrules.SkopeRules" href="#imodels.skrules.SkopeRules">SkopeRules</a></code></h4>
<ul class="two-column">
<li><code><a title="imodels.skrules.SkopeRules.decision_function" href="#imodels.skrules.SkopeRules.decision_function">decision_function</a></code></li>
<li><code><a title="imodels.skrules.SkopeRules.deduplicate" href="#imodels.skrules.SkopeRules.deduplicate">deduplicate</a></code></li>
<li><code><a title="imodels.skrules.SkopeRules.f1_score" href="#imodels.skrules.SkopeRules.f1_score">f1_score</a></code></li>
<li><code><a title="imodels.skrules.SkopeRules.fit" href="#imodels.skrules.SkopeRules.fit">fit</a></code></li>
<li><code><a title="imodels.skrules.SkopeRules.predict" href="#imodels.skrules.SkopeRules.predict">predict</a></code></li>
<li><code><a title="imodels.skrules.SkopeRules.predict_proba" href="#imodels.skrules.SkopeRules.predict_proba">predict_proba</a></code></li>
<li><code><a title="imodels.skrules.SkopeRules.predict_top_rules" href="#imodels.skrules.SkopeRules.predict_top_rules">predict_top_rules</a></code></li>
<li><code><a title="imodels.skrules.SkopeRules.rules_vote" href="#imodels.skrules.SkopeRules.rules_vote">rules_vote</a></code></li>
<li><code><a title="imodels.skrules.SkopeRules.score_top_rules" href="#imodels.skrules.SkopeRules.score_top_rules">score_top_rules</a></code></li>
</ul>
</li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc"><cite>pdoc</cite> 0.7.2</a>.</p>
</footer>
<script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/highlight.min.js"></script>
<script>hljs.initHighlightingOnLoad()</script>
</body>
</html>